{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png\" width = 300, align = \"center\"></a>\n",
    "\n",
    "<h1 align=center><font size = 5> MUSIC GENRE CLASSIFICATION USING TENSORFLOW - PART 1</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<font size = 3><strong>In this notebook we provide a brief overview of the music project, explain the data set, and help you extract features from the raw data</strong></font>\n",
    "<br>\n",
    "- <p><a href=\"#ref2\">Overview of the project</a></p>\n",
    "- <p><a href=\"#ref3\">Brief explanation of the data set</a></p>\n",
    "- <p><a href=\"#ref4\">Feature extraction</a></p>\n",
    "<p></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2\"></a>\n",
    "# Overview of the project\n",
    "\n",
    "In this project, our objective is to classify a sample of music into a specific genre using convolutional networks. Similar to the way images of handwritten numbers are classified into any one of 10 digit classes in the MNIST data set, here we take samples/excerpts of music and classify these excerpts into any one of 9 genres - Alternative, Blues, Electronic, Folk/Country, Funk/Soul/R&B, Jazz, Pop, Rap/Hiphop, Rock.\n",
    "\n",
    "The project is broken into two modules. In this first of two modules, we will download the data set, understand how the data set is structured, and extract relevant features. These features will be used later in the second module to train convolutional networks for the purpose of genre classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"ref3\"></a>\n",
    "# Brief explanation of the data set\n",
    "\n",
    "We will use the [Music Audio Benchmark Data Set](http://www-ai.cs.uni-dortmund.de/audio.html) for this project, created by [Homburg et al. (2005)](http://www-ai.cs.uni-dortmund.de/audio.html) and [Mierswa et al. (2005)](http://www-ai.cs.uni-dortmund.de/audio.html). The data set contains 1886 song excerpts, from 9 genres (listed above), encoded in mp3 format. The frequency and bitrate of these files are 44,100 Hz and 128 kb respectively. Each excerpt is 10 seconds in duration. For ease of use, the data has already been imported and split for training the networks and testing how the networks perform. The train:test data split is approximately 80:20, respectively.\n",
    "\n",
    "In the next section, we will extract the necessary features for genre classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref4\"></a>\n",
    "# Feature extraction\n",
    "\n",
    "In a computer vision problem such as handwritten digit recognition using MNIST, we can use features such as pixel values or RGB values. Audio signals, particularly music excerpts, are temporal and more complex. As such, they offer a rich variety of features ranging from low-level acoustic to high-level musical features. Low-level features could be spectral and spectrotemporal features while high-level features could be tempo, rhythm, key, pitch, and harmonic information. Since deep learning is primarily designed for feature learning, it makes sense to use lower level features in this project. So, we will use mel frequency cepstral coefficients (MFCCs) as our features.\n",
    "\n",
    "MFCCs are supposed to mimic the logarithmic perception of loudness and pitch of the human auditory system. Although MFCCs have been used mostly in speech recognition, they have become very popular as features for problems within the music information retrieval community. For a deeper understanding of MFCCs, we highly recommend going through the following sources: 1) [This excellent video](https://archive.org/details/SpectrogramCepstrumAndMel-frequency_636522) by Kishore Prahallad, and 2) this short [explanatory tutorial](http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/).\n",
    "\n",
    "We will use [Librosa](http://librosa.github.io/librosa/), a python package for music and audio analysis, for extracting MFCC features from music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put this before all imports for the librosa package    \n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the links for all the Music Training data (part1) and Music Testing data (part2) \n",
    "!wget -q -c --progress=bar:force https://ibm.box.com/shared/static/zv4o44z5wzpyqgjvq07ed00dqlpb7mho.gz -O part1.tar.gz\n",
    "!wget -q -c --progress=bar:force https://ibm.box.com/shared/static/mfa7hosvxe179713jgnpt2m7z1nwrxc1.gz -O part2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the downloaded tar zipped files\n",
    "!ls -lah part*.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa as lr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tar files for the training and testing data\n",
    "\n",
    "part1Tar = tarfile.open(name=\"part1.tar.gz\", mode=\"r:*\")\n",
    "part2Tar = tarfile.open(name=\"part2.tar.gz\", mode=\"r:*\")\n",
    "\n",
    "allTrainFiles = part1Tar.getnames()\n",
    "allTestFiles = part2Tar.getnames()\n",
    "\n",
    "csvTrainFiles = [f for f in allTrainFiles if \".csv\" in f and os.path.basename(f)[0]!=\".\"]\n",
    "mp3TrainFiles = [f for f in allTrainFiles if \".mp3\" in f and os.path.basename(f)[0]!=\".\"]\n",
    "\n",
    "csvTestFiles = [f for f in allTestFiles if \".csv\" in f and os.path.basename(f)[0]!=\".\"]\n",
    "mp3TestFiles = [f for f in allTestFiles if \".mp3\" in f and os.path.basename(f)[0]!=\".\"]\n",
    "\n",
    "print \"Total CSV Files in Training Set: {0}\".format(len(csvTrainFiles))\n",
    "print \"Total MP3 Files in Training Set: {0}\".format(len(mp3TrainFiles))\n",
    "print(\"\\n\")\n",
    "print \"Total CSV Files in Test Set: {0}\".format(len(csvTestFiles))\n",
    "print \"Total MP3 Files in Test Set: {0}\".format(len(mp3TestFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parse CSV files first to get the genre labels for each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain genre labels for the training data\n",
    "# Store data into a DataFrame\n",
    "_framesTrain = []\n",
    "for f in csvTrainFiles:    \n",
    "    fObjTrain = part1Tar.extractfile(f)    \n",
    "    _framesTrain.append(pd.read_csv(fObjTrain))\n",
    "    \n",
    "trainLabels = pd.concat(_framesTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain genre labels for the test data\n",
    "# Store data into a DataFrame\n",
    "_framesTest = []\n",
    "for f in csvTestFiles:    \n",
    "    fObjTest = part2Tar.extractfile(f)    \n",
    "    _framesTest.append(pd.read_csv(fObjTest))\n",
    "    \n",
    "testLabels = pd.concat(_framesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few samples of these dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training data - Print song names and genre labels of first 5 rows\n",
    "trainLabels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data - Print song names and genre labels of first 5 rows\n",
    "testLabels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map music genre categories to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(set(trainLabels.Genre))\n",
    "categories.sort()\n",
    "categories = pd.Series(categories)\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse each mp3 file to compute MFCC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we get the feature data for the training set\n",
    "mfcc = []\n",
    "y = []\n",
    "sr = []\n",
    "labels = []\n",
    "labelNums = []\n",
    "\n",
    "# Librosa loads and decodes the audio as a time series 'y', \n",
    "# represented as a one-dimensional NumPy floating point array. \n",
    "# The variable 'sr' contains the sampling rate of 'y', i.e., the number of samples per second of audio. \n",
    "# By default, all audio is mixed to mono and resampled to 22050 Hz at load time. \n",
    "\n",
    "dataSetTrain = pd.DataFrame(columns=['Song','mfcc','y','sr','label','Genre'])\n",
    "\n",
    "for f in mp3TrainFiles:\n",
    "    fObjTrain = part1Tar.extract(f)\n",
    "    \n",
    "    _songTrain = os.path.basename(f)\n",
    "    _yTrain, _srTrain = lr.load(f)\n",
    "    _mfccTrain = lr.feature.mfcc(y=_yTrain, sr=_srTrain, n_mfcc=40)   \n",
    "    _labelTrain = trainLabels.Genre[trainLabels.Song==_songTrain].values[0]\n",
    "    _labelNumTrain = categories[categories==_labelTrain].index[0]\n",
    "\n",
    "    mfcc.append(_mfccTrain)\n",
    "    y.append(_yTrain)\n",
    "    sr.append(_srTrain)\n",
    "    labels.append(_labelTrain) \n",
    "    labelNums.append(_labelNumTrain)\n",
    "    \n",
    "    _dfTrain = pd.DataFrame([[_songTrain, _mfccTrain, _yTrain , _srTrain, _labelNumTrain, \n",
    "                              _labelTrain]], columns=['Song','mfcc','y','sr','label','Genre'])\n",
    "    dataSetTrain = dataSetTrain.append(_dfTrain, ignore_index=True)    \n",
    "    print os.path.basename(_dfTrain.Song.values[-1]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the last 5 rows of the Training data\n",
    "dataSetTrain.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a random excerpt back from the training set! (randomized)\n",
    "sample = np.random.randint(len(dataSetTrain.index))\n",
    "\n",
    "print \"Playing:\", dataSetTrain.Song[sample]\n",
    "print \"Genre: \", dataSetTrain.Genre[sample]\n",
    "print \"mfcc: \", dataSetTrain.mfcc.values[sample]\n",
    "\n",
    "IPython.display.Audio(data=dataSetTrain.y[sample], rate=sr[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the MFCCs for the randomly selected excerpt\n",
    "%matplotlib inline\n",
    "lr.display.specshow(dataSetTrain.mfcc.values[sample], x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature data for the test set\n",
    "mfcc = []\n",
    "y = []\n",
    "sr = []\n",
    "labels = []\n",
    "labelNums = []\n",
    "\n",
    "# Librosa loads and decodes the audio as a time series 'y', \n",
    "# represented as a one-dimensional NumPy floating point array. \n",
    "# The variable 'sr' contains the sampling rate of 'y', i.e., the number of samples per second of audio. \n",
    "# By default, all audio is mixed to mono and resampled to 22050 Hz at load time. \n",
    "dataSetTest = pd.DataFrame(columns=['Song','mfcc','y','sr','label','Genre'])\n",
    "\n",
    "for f in mp3TestFiles:\n",
    "    fObjTest = part2Tar.extract(f)\n",
    "    \n",
    "    _songTest = os.path.basename(f)\n",
    "    _yTest, _srTest = lr.load(f)\n",
    "    _mfccTest = lr.feature.mfcc(y=_yTest, sr=_srTest, n_mfcc=40)   \n",
    "    _labelTest = testLabels.Genre[testLabels.Song==_songTest].values[0]\n",
    "    _labelNumTest = categories[categories==_labelTest].index[0]\n",
    "\n",
    "    mfcc.append(_mfccTest)\n",
    "    y.append(_yTest)\n",
    "    sr.append(_srTest)\n",
    "    labels.append(_labelTest) \n",
    "    labelNums.append(_labelNumTest)\n",
    "    \n",
    "    _dfTest = pd.DataFrame([[_songTest, _mfccTest, _yTest, _srTest, _labelNumTest, \n",
    "                              _labelTest]], columns=['Song','mfcc','y','sr','label','Genre'])\n",
    "    dataSetTest = dataSetTest.append(_dfTest, ignore_index=True)    \n",
    "    print os.path.basename(_dfTest.Song.values[-1]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the last 5 rows of the Test data\n",
    "dataSetTest.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the training feature DataFrame as a CSV file\n",
    "dataSetTrain.to_csv('trainMFCC.csv')\n",
    "\n",
    "# Save the test feature DataFrame as a CSV file\n",
    "dataSetTest.to_csv('testMFCC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick test to see if the CSV files can be re-imported as pandas DataFrames\n",
    "reimp = pd.read_csv('trainMFCC.csv')\n",
    "reimp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at [Data Science Experience](https://cocl.us/ML0120EN_DSX)This is the end of this lesson. Hopefully, now you have a deeper and intuitive understanding regarding the LSTM model. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
